#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üîç –î–ï–ë–ê–ì –°–¢–†–£–ö–¢–£–†–´ –°–¢–†–ê–ù–ò–¶–´
–°–∞–π—Ç: ecopackpro.ru
–¶–µ–ª—å: –ù–∞–π—Ç–∏ –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
"""

import requests
import re
from datetime import datetime
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def debug_page_structure(url):
    """–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
    logging.info(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    })
    
    try:
        response = session.get(url, timeout=10)
        content = response.text
        
        logging.info(f"üìä –†–∞–∑–º–µ—Ä —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(content)} —Å–∏–º–≤–æ–ª–æ–≤")
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        link_pattern = r'href="([^"]+)"'
        all_links = re.findall(link_pattern, content)
        
        logging.info(f"üîó –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ: {len(all_links)}")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
        internal_links = []
        external_links = []
        
        for link in all_links:
            if link.startswith('/') or 'ecopackpro.ru' in link:
                if not link.startswith('http'):
                    link = 'https://ecopackpro.ru' + link
                internal_links.append(link)
            elif link.startswith('http'):
                external_links.append(link)
        
        logging.info(f"üè† –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {len(internal_links)}")
        logging.info(f"üåê –í–Ω–µ—à–Ω–∏—Ö —Å—Å—ã–ª–æ–∫: {len(external_links)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –≤—Å–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏
        logging.info("üìã –í–°–ï –í–ù–£–¢–†–ï–ù–ù–ò–ï –°–°–´–õ–ö–ò –ù–ê –°–¢–†–ê–ù–ò–¶–ï:")
        for i, link in enumerate(internal_links, 1):
            logging.info(f"{i:3d}. {link}")
        
        # –ò—â–µ–º –ø–æ–¥–≤–∞–ª —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏
        logging.info("\nüîç –ü–û–ò–°–ö –ü–û–î–í–ê–õ–ê –†–ê–ó–ù–´–ú–ò –°–ü–û–°–û–ë–ê–ú–ò:")
        
        # 1. –ò—â–µ–º footer
        footer_match = re.search(r'<footer[^>]*>(.*?)</footer>', content, re.DOTALL | re.IGNORECASE)
        if footer_match:
            footer_content = footer_match.group(1)
            logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω <footer>: {len(footer_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –≤ footer
            footer_links = re.findall(link_pattern, footer_content)
            logging.info(f"üîó –°—Å—ã–ª–æ–∫ –≤ footer: {len(footer_links)}")
            for link in footer_links:
                logging.info(f"   - {link}")
        else:
            logging.info("‚ùå <footer> –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # 2. –ò—â–µ–º –∫–ª–∞—Å—Å footer
        footer_class_match = re.search(r'<div[^>]*class="[^"]*footer[^"]*"[^>]*>(.*?)</div>', content, re.DOTALL | re.IGNORECASE)
        if footer_class_match:
            footer_content = footer_class_match.group(1)
            logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω div —Å –∫–ª–∞—Å—Å–æ–º footer: {len(footer_content)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            footer_links = re.findall(link_pattern, footer_content)
            logging.info(f"üîó –°—Å—ã–ª–æ–∫ –≤ div.footer: {len(footer_links)}")
            for link in footer_links:
                logging.info(f"   - {link}")
        else:
            logging.info("‚ùå div —Å –∫–ª–∞—Å—Å–æ–º footer –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # 3. –ò—â–µ–º –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3000 —Å–∏–º–≤–æ–ª–æ–≤)
        end_content = content[-3000:]
        logging.info(f"üìÑ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω–µ—Ü —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 3000 —Å–∏–º–≤–æ–ª–æ–≤)")
        
        end_links = re.findall(link_pattern, end_content)
        logging.info(f"üîó –°—Å—ã–ª–æ–∫ –≤ –∫–æ–Ω—Ü–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {len(end_links)}")
        for link in end_links:
            logging.info(f"   - {link}")
        
        # 4. –ò—â–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
        nav_patterns = [
            r'<nav[^>]*>(.*?)</nav>',
            r'<div[^>]*class="[^"]*nav[^"]*"[^>]*>(.*?)</div>',
            r'<div[^>]*class="[^"]*menu[^"]*"[^>]*>(.*?)</div>'
        ]
        
        for pattern in nav_patterns:
            nav_match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if nav_match:
                nav_content = nav_match.group(1)
                nav_links = re.findall(link_pattern, nav_content)
                if nav_links:
                    logging.info(f"üß≠ –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ ({pattern}): {len(nav_links)}")
                    for link in nav_links:
                        logging.info(f"   - {link}")
        
        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        with open('/var/www/fastuser/data/www/ecopackpro.ru/page_content.html', 'w', encoding='utf-8') as f:
            f.write(content)
        logging.info("üíæ HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ page_content.html")
        
        return internal_links
        
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        return []

if __name__ == "__main__":
    url = "https://ecopackpro.ru/korobki-dlya-otpravki/"
    debug_page_structure(url)



